{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import codecs\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporal aggregation interval\n",
    "AGGR_TIME = 10*60 #only for sociopatterns\n",
    "# AGGR_TIME = 1 #only for OpenABM\n",
    "\n",
    "# temporal coupling weight\n",
    "temp_edge_weight_list = [1]#,2,4,8,16]\n",
    "\n",
    "# temporal coupling repr\n",
    "rep_time_list = ['Non']\n",
    "#rep_time_list = ['Inverse']\n",
    "\n",
    "# dataset_list = ['LyonSchool', 'InVS15', 'SFHH', 'LH10', 'Thiers13']\n",
    "dataset_list = ['LyonSchool']\n",
    "# dataset_list = ['InVS15']\n",
    "# dataset_list = ['SFHH']\n",
    "# dataset_list = ['LH10']\n",
    "# dataset_list = ['Thiers13']\n",
    "\n",
    "# dataset_list = ['OpenABM-Covid19-Interactions-5k-20']\n",
    "# dataset_list = ['OpenABM-Covid19-Interactions-2k-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_temp_data(dataset):\n",
    "    if dataset.split('-')[0]=='OpenABM':\n",
    "        df_temp_net = pd.read_csv(('../data/Data_OpenABM/%s.tar.gz' % dataset))\\\n",
    "                                .rename(columns={'ID_1':'i', 'ID_2':'j', 'time':'t'})\n",
    "    else:\n",
    "        df_temp_net = pd.read_csv(('../data/Data_SocioPatterns_20s_nonights/tij_%s.dat_nonights.dat' % dataset),\n",
    "                        sep = '\\t', header = None,\n",
    "                        names = ['t', 'i', 'j'])\n",
    "    # compute slice each contact event belongs to\n",
    "    df_temp_net.sort_values('t', inplace=True)\n",
    "    df_temp_net.loc[:,'tslice'] = np.floor((df_temp_net.t - df_temp_net.t.iloc[0]) / AGGR_TIME)\n",
    "    # group over (slice, i, j), and compute number of contacts within time slice,\n",
    "    # regarded as \"weight\" for contacts in each time slice\n",
    "    \n",
    "    df_temp_net = df_temp_net[df_temp_net.i!=df_temp_net.j]\n",
    "    s = df_temp_net['i'] > df_temp_net['j']\n",
    "    df_temp_net.loc[s, ['i','j']] = df_temp_net.loc[s, ['j','i']].values\n",
    "    df_temp_net.drop_duplicates(['t','i','j'], inplace=True)\n",
    "    \n",
    "    s_temp_net = df_temp_net.groupby(['tslice','i','j']).size().rename('weight')\n",
    "    \n",
    "    # times for all temporal slices, note that it may have a big gap (return to home)\n",
    "    partial_times = sorted(list(s_temp_net.index.levels[0]))\n",
    "\n",
    "    # convenience: dataframe version of the series above\n",
    "    df_tnet = s_temp_net.reset_index()\n",
    "\n",
    "    #make list of ID and active time\n",
    "    #sort the embedding result by using this list.\n",
    "    pat_active_time = [[('%d-%d' % (e[1], e[0])), ('%d-%d' % (e[2], e[0]))] for e, weight in s_temp_net.iteritems()]\n",
    "    pat_active_time = list(set([item for sublist in pat_active_time for item in sublist]))\n",
    "\n",
    "    target_dir = '../preprocessed/SupraAdjacencyMatrix/%s' %dataset\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    pat_active_time_file_name = \"../preprocessed/SupraAdjacencyMatrix/%s/PatActiveTimeAggtime%d.pkl\" % (dataset, AGGR_TIME)\n",
    "    if (0==os.path.isfile(pat_active_time_file_name)):\n",
    "        pickle.dump(pat_active_time, open(pat_active_time_file_name, \"wb\" ) )\n",
    "    \n",
    "    return partial_times, df_temp_net, df_tnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44820 15873 15500\n"
     ]
    }
   ],
   "source": [
    "for dataset in dataset_list:\n",
    "    _, df_unfold_net, df_tnet = load_temp_data(dataset)\n",
    "    \n",
    "    df_tree = df_tnet.groupby('tslice')\\\n",
    "                .apply(lambda x: nx.Graph(list(x[['i', 'j']].values)))\\\n",
    "                .map(lambda g: np.array(list(nx.minimum_spanning_edges(g, data=False))))\\\n",
    "                .to_frame('edge').reset_index()#.explode('edge')\n",
    "    \n",
    "    edges = np.sort(np.concatenate([e for e in df_tree.edge]), axis=1)\n",
    "    df_tree = df_tree.explode('edge')\n",
    "    df_tree['i'] = edges[:,0]\n",
    "    df_tree['j'] = edges[:,1]\n",
    "    del df_tree['edge']\n",
    "    \n",
    "    df_merge = df_tree.merge(df_tnet.reset_index(), on=['tslice', 'i', 'j'], how='outer', indicator=True)\n",
    "    idx_samp = df_merge[df_merge._merge=='right_only']['index'].values\n",
    "    \n",
    "    start_size = df_tnet.shape[0]\n",
    "    tree_size = df_tree.shape[0]\n",
    "    final_size = int(0.7*start_size)\n",
    "    samp_size = final_size - tree_size\n",
    "    assert(samp_size>0)\n",
    "    print(start_size, tree_size, samp_size)\n",
    "    \n",
    "    for itr_split in range(1):\n",
    "        \n",
    "        df_samp = df_tnet.iloc[idx_samp].sample(n=samp_size, replace=False, random_state=itr_split)\n",
    "        df_samp = pd.concat((df_tree, df_samp)).reset_index(drop=True)\n",
    "        \n",
    "        df_save = df_unfold_net.merge(df_samp, on=['i', 'j', 'tslice']).loc[:,['t','i','j']]\n",
    "        target_dir = '../preprocessed/RemovedLinksTempNet/%s' % dataset\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        df_save.to_csv(target_dir + '/tij_%s_7030_%d.csv.gz' % (dataset, itr_split), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
