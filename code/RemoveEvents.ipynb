{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import codecs\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporal aggregation interval\n",
    "AGGR_TIME = 10*60\n",
    "\n",
    "# temporal coupling weight\n",
    "temp_edge_weight_list = [1]#,2,4,8,16]\n",
    "\n",
    "# temporal coupling repr\n",
    "rep_time_list = ['Non']\n",
    "#rep_time_list = ['Inverse']\n",
    "\n",
    "# dataset_list = ['LyonSchool', 'InVS15', 'SFHH', 'LH10', 'Thiers13']\n",
    "dataset_list = ['LyonSchool']\n",
    "# dataset_list = ['InVS15']\n",
    "# dataset_list = ['SFHH']\n",
    "# dataset_list = ['LH10']\n",
    "# dataset_list = ['Thiers13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_temp_data(dataset):\n",
    "    df_temp_net = pd.read_csv(('../data/Data_SocioPatterns_20s_nonights/tij_%s.dat_nonights.dat' % dataset),\n",
    "                        sep = '\\t', header = None,\n",
    "                        names = ['t', 'i', 'j'])\n",
    "    # compute slice each contact event belongs to\n",
    "    df_temp_net.loc[:,'tslice'] = np.floor((df_temp_net.t - df_temp_net.t.iloc[0]) / AGGR_TIME)\n",
    "    # group over (slice, i, j), and compute number of contacts within time slice,\n",
    "    # regarded as \"weight\" for contacts in each time slice\n",
    "    \n",
    "    df_temp_net = df_temp_net[df_temp_net.i!=df_temp_net.j]\n",
    "    s = df_temp_net['i'] > df_temp_net['j']\n",
    "    df_temp_net.loc[s, ['i','j']] = df_temp_net.loc[s, ['j','i']].values\n",
    "    df_temp_net.drop_duplicates(['t','i','j'], inplace=True)\n",
    "    \n",
    "    s_temp_net = df_temp_net.groupby(['tslice','i','j']).size().rename('weight')\n",
    "    \n",
    "    # times for all temporal slices, note that it may have a big gap (return to home)\n",
    "    partial_times = sorted(list(s_temp_net.index.levels[0]))\n",
    "\n",
    "    # convenience: dataframe version of the series above\n",
    "    df_tnet = s_temp_net.reset_index()\n",
    "\n",
    "    #make list of ID and active time\n",
    "    #sort the embedding result by using this list.\n",
    "    pat_active_time = [[('%d-%d' % (e[1], e[0])), ('%d-%d' % (e[2], e[0]))] for e, weight in s_temp_net.iteritems()]\n",
    "    pat_active_time = list(set([item for sublist in pat_active_time for item in sublist]))\n",
    "\n",
    "    target_dir = '../preprocessed/SupraAdjacencyMatrix/%s' %dataset\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    pat_active_time_file_name = \"../preprocessed/SupraAdjacencyMatrix/%s/PatActiveTimeAggtime%d.pkl\" % (dataset, AGGR_TIME)\n",
    "    if (0==os.path.isfile(pat_active_time_file_name)):\n",
    "        pickle.dump(pat_active_time, open(pat_active_time_file_name, \"wb\" ) )\n",
    "    \n",
    "    return partial_times, df_temp_net, df_tnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samp_func(df, random_state):\n",
    "    act = len(set(df.i.values) | set(df.j.values))\n",
    "    todrop = df[df.r].index\n",
    "    \n",
    "    if len(todrop)==0:\n",
    "        return df\n",
    "    \n",
    "    i = random_state.choice(todrop, 1)[0]\n",
    "    df_samp = df.drop(index=i)\n",
    "    act_samp = len(set(df_samp.i.values) | set(df_samp.j.values))\n",
    "    \n",
    "    if act_samp == act:\n",
    "        return df_samp\n",
    "    else:\n",
    "        df.at[i,'r'] = False \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in dataset_list:\n",
    "    _, df_unfold_net, df_tnet = load_temp_data(dataset)\n",
    "    \n",
    "    for itr_split in range(1):\n",
    "        rs = np.random.RandomState(itr_split)\n",
    "        df_samp = df_tnet.copy()\n",
    "        df_samp['r'] = True\n",
    "        start_size = df_samp.shape[0]\n",
    "        while True:\n",
    "            prev_size = df_samp.shape[0]\n",
    "            df_samp = df_samp.groupby('tslice').apply(lambda x: samp_func(x, rs)).reset_index(drop=True)\n",
    "            if (df_samp.shape[0] <= 0.7*start_size) or (df_samp.shape[0]==prev_size):\n",
    "                break\n",
    "        del df_samp['r']\n",
    "        df_save = df_unfold_net.merge(df_samp, on=['i', 'j', 'tslice']).loc[:,['t','i','j']]\n",
    "        target_dir = '../preprocessed/RemovedLinksTempNet/%s' % dataset\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        df_save.to_csv(target_dir + '/tij_%s_7030_%d.csv.gz' % (dataset, itr_split), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
